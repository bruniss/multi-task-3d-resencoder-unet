tr_setup:
  model_name: webk_ink_test2                    # required [str] this is the name your model will checkpoint will be saved as
  vram_max: 22000                    # required if autoconfigure is true, else optional-- the amount in MB you want the model to use
  autoconfigure: true               # optional, true -- if true, the network will attempt to set some reasonable defaults based on your vram_max
  tr_val_split: 0.90                 # optional[float] the percentage of your total dataset used for training, with the other part being used for val
  dilate_label: false                # optional, false [bool] if true, will apply a small dilation to your labels using a 3x3 spherical kernel -- will skip tasks named "normals"
  # checkpoint_path: "./ink_157.pth" # optional, none [str] if provided, will load the provided checkpoint and begin training from there
  load_weights_only: false           # optional, false [bool] if true, will not load the optimizer, scheduler, or epoch state from the model -- set true to fine-tune
  ckpt_out_base: "./checkpoints"     # [str] the path the model checkpoint is saved to
  tensorboard_log_dir: "./logs"      # optional, './' [str] the path the tensorboard logs will be stored to

tr_config:
  optimizer: "AdamW"            # optional, AdamW [str] the optimizer to use during training. currently only AdamW and SGD are provided.
  patch_size: [10, 128, 128]    # optional [list] patch size for training
  batch_size: 2                # optional [int] batch size for training
  max_steps_per_epoch: 100
  max_val_steps_per_epoch: 25
  max_epoch: 500                # optional, 500 [int] the maximum number of epochs to train for

model_config:
  use_timm_encoder: false
  timm_encoder_class: null

dataset_config:
  min_bbox_percent: 0.20      # optional, 0.97 [float] a percentage of the patch size that must be encompassed by a bbox containing all the labels in the patch
  min_labeled_ratio: 0.20     # optional, 0.15 [float] a percentage of the above bbox that must contain labeled data (ie: the density of the labels)
  use_cache: true             # optional, True [bool] whether to store a patch position cache. strongly recommended
  cache_file: "./patch_cache" # optional, './' the location to store the cache

  # you do not need these two if you use the wk zarr stream link , only if you want to use the api (i would not)
  wk_url: "http://dl.ash2txt.org:8080/"
  wk_token: ""

  targets:
    ink:
      in_channels: 1
      out_channels: 1
      volumes:
        - label_volume: "678d7e580100001956ee6494"
          label_id: "Volume"
          data_volume: "678d7e580100001956ee6494"
          data_id: "first_letters_segment"
          spacing: [1, 1, 1]
          format: 'wk_api'
          shape: 'xyz'


      activation: "none"     # optional, none [str] the activation type you would like your model to perform during training, options are: Sigmoid , Softmax, None.
      weight: 1              # optional, 1 [float] the weight applied to the task, as a percentage. this is multiplied by the loss value during training, so 1 is 100%
      loss_fn: "BCEWithLogitsLoss" # optional, 'BCEDiceLoss' [str] the loss you would like to use from the loss_fn_map in train.py. to add losses, simply add to the mapping in train.py
#      loss_kwargs:           # optional, none - any keyword arguments you would like to pass to your loss function, each on its own line
#        alpha: 0.5
#        beta: 0.5


inference_config:                           # these parameters only apply on inference
  checkpoint_path: "./checkpoints/ckpt.pth" # the checkpoint to load
  num_dataloader_workers: 12                # number of workers for the inference dataloader
  input_path: "./data/0901/layers.zarr"     # [str] input volume, CURRENTLY MUST BE ZARR
  input_format: "zarr"                      # not currently implemented, all data is zarr
  output_path: "./outputs/0901"              # data is saved to zarr in the output dir as predictions.zarr
  output_format: "zarr"                     # currently does nothing , can only be zarr
  output_type: "np.uint8"                   # currently does nothing
  output_targets: ["ink"]                   # optional, if none all targets will be output
  load_all: true                            # optional, false - if true, will load entire input_path volume into memory
  overlap: 0.1                              # optional, .01 - the percentage of overlap each patch should have
  patch_size: [128, 128, 128]               # this can be different than your training setup , but may negatively impact performance
  batch_size: 4                             # can typically be higher than your train batch size, watch vram
  targets:                                  # must be the targets you trained with , including num channels and name.
    - ink:                                    # to the number of channels your model outputs
        channels: 1
        activation: "none"
        weight: 1
