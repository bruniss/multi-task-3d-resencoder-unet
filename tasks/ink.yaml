# YOU DO NOT HAVE TO PROVIDE VALUES IN MOST OF THESE FIELDS

tr_setup:
  model_name: ink                    # required [str] this is the name your model will checkpoint will be saved as
  vram_max: 22000                    # required if autoconfigure is true, else optional-- the amount in MB you want the model to use
  autoconfigure: true               # optional, true -- if true, the network will attempt to set some reasonable defaults based on your vram_max
  tr_val_split: 0.90                 # optional[float] the percentage of your total dataset used for training, with the other part being used for val
  dilate_label: false                # optional, false [bool] if true, will apply a small dilation to your labels using a 3x3 spherical kernel -- will skip tasks named "normals"
  # checkpoint_path: "./ink_157.pth" # optional, none [str] if provided, will load the provided checkpoint and begin training from there
  load_weights_only: false           # optional, false [bool] if true, will not load the optimizer, scheduler, or epoch state from the model -- set true to fine-tune
  ckpt_out_base: "./checkpoints/ink2"     # [str] the path the model checkpoint is saved to
  tensorboard_log_dir: "./logs"      # optional, './' [str] the path the tensorboard logs will be stored to

tr_config:
  optimizer: "AdamW"            # optional, AdamW [str] the optimizer to use during training. currently only AdamW and SGD are provided.
  initial_lr: 0.001             # optional initial learning rate
  weight_decay: 0.0001             # optional, 0.0001 [float]
  gradient_accumulation: 1      # optional, 1 [int] if 1, no accumulation, if >1 will accumulate this many 'batches' each batch to simulate larger batch size.
  num_dataloader_workers: 12    # optional, 4 [int]
  patch_size: [14, 256, 256]    # optional [list] patch size for training
  batch_size: 3              # optional [int] batch size for training
  max_steps_per_epoch: 500      # optional, 500 [int] the number of batches seen by the model each epoch
  max_val_steps_per_epoch: 25   # optional, 25 [int] the number of batches seen in validation each epoch
  max_epoch: 500                # optional, 500 [int] the maximum number of epochs to train for
  ignore_label: null            # [NOT YET IMPLEMENTED] if you have an ignore label, you can set it here and loss will not be computed against it
  loss_only_on_label: false     # [NOT YET IMPLEMENTED]

model_config:
  use_timm_encoder: false
  timm_encoder_class: null
  basic_encoder_block: "ResidualBlock"                      # or ConvBlock
  basic_decoder_block: "ConvBlock"
  features_per_stage: [32, 64, 128, 256, 320, 320]  # optional, [list] or [int] -- if an int is provided will automatically double each layer
  num_stages: 6                                     # number of layers
  n_blocks_per_stage: [1, 3, 4, 6, 6, 6]            #
  n_conv_per_stage_decoder: [1, 1, 1, 1, 1]         # should be n_blocks_per_stage - 1
  bottleneck_block: 'BasicBlockD'                   # could be BottleneckD but just leave this as-is
  op_dims: 3                                        # this is the amount of dimensions of your input data, must be 1 , 2, or 3
  kernel_sizes: [3, 3, 3, 3, 3, 3]                                 # conv kernel size (3, 3, 3) for each block
  pool_op: 'nn.MaxPool3d'                           # currently defaults to this if 3d, MaxPool2d if 2d
  conv_bias: true                                  # should be true
  norm_op: 'nn.InstanceNorm3d'                      # currenlty defaults to instancenorm3d for 3d and 2d for 2d, this doesnt change it
  dropout_op: 'nn.Dropout3d'
  dropout_op_kwargs: {'p': 0.0}
  nonlin: 'nn.LeakyReLU'
  nonlin_kwargs: {'inplace': True}
  strides: [1, 2, 2, 2, 2, 2]                   # aka dilation
  return_skips: True
  do_stem: True
  stem_channels: None
  bottleneck_channels: null
  stochastic_depth_p: 0.0
  squeeze_excitation: true

dataset_config:
  # the dataset config is where we build the targets data and the labeled data. you can use any number of inputs and labels.
  # the dataset is configured to search a zarr volume of labels for valid patches. we consider a valid patch to be one that meets the
  # criteria set by min_bbox_percent and min_labeled_ratio. the z, y, x starting positions of these are saved to a json so we only
  # have to compute this once. on large volumes, this can take a long time.
  # ref label:
      # used for the valid patch finding. the name of these refers to the volume within volume paths. in the below example
      # "ink" is referring to the ink volume, defined one line above. you can provide any number of volume paths.

  min_bbox_percent: 0.20 # optional, 0.97 [float] a percentage of the patch size that must be encompassed by a bbox containing all the labels in the patch
  min_labeled_ratio: 0.20 # optional, 0.15 [float] a percentage of the above bbox that must contain labeled data (ie: the density of the labels)
  use_cache: true # optional, True [bool] whether to store a patch position cache. strongly recommended
  cache_file: "/patch_caches/6/" # optional, './' the location to store the cache
  volume_paths: # these are your input volumes. each must have input volume path, target volume path (from your targets defined in targets),

    - input: "/home/sean/Desktop/s1_segments/1321.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/1321.zarr/inklabels.zarr"
      ref_label: "ink"

    - input: "/home/sean/Desktop/s1_segments/1619.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/1619.zarr/inklabels.zarr"
      ref_label: "ink"

    - input: "/home/sean/Desktop/s1_segments/3336.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/3336.zarr/inklabels.zarr"
      ref_label: "ink"

    - input: "/home/sean/Desktop/s1_segments/4423.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/4423.zarr/inklabels.zarr"
      ref_label: "ink"

    - input: "/home/sean/Desktop/s1_segments/5753.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/5753.zarr/inklabels.zarr"
      ref_label: "ink"

    - input: "/home/sean/Desktop/s1_segments/51002.zarr/layers.zarr"
      ink: "/home/sean/Desktop/s1_segments/51002.zarr/inklabels.zarr"
      ref_label: "ink"

  targets:
    # the targets provided here are how the model is configured. for each entry here, the model will construct an additional
    # decoder path, with the specified number of channels.
    ink:
      channels: 1            # required [int] the number of channels in your input -- your input should be in shape z, y, x, c
      activation: "none"     # optional, none [str] the activation type you would like your model to perform during training, options are: Sigmoid , Softmax, None.
                             # be careful you do not introduce a double activation if your loss function also applies one

      weight: 1              # optional, 1 [float] the weight applied to the task, as a percentage. this is multiplied by the loss value during training, so 1 is 100%
                             # if you have multiple tasks, these do not need to equal 1. this is the default. ex: weight: 0.75

      loss_fn: "BCEDiceLoss" # optional, 'BCEDiceLoss' [str] the loss you would like to use from the loss_fn_map in train.py. to add losses, simply add to the mapping in train.py
      loss_kwargs:           # optional, none - any keyword arguments you would like to pass to your loss function, each on its own line
        alpha: 0.5
        beta: 0.5

inference_config:
  patch_size: [14, 128, 128] # optional - patch size to use for inference. this can be different than what you trained with, but might hurt performance
  batch_size: 8              # optional - this typically can be larger for inference than training
  checkpoint_path: "/home/sean/Documents/GitHub/multi-task-3d-unet/checkpoints/ink_30.pth" # the checkpoint to load
  num_dataloader_workers: 12
  input_path: "/home/sean/Desktop/s1_segments/0901.zarr/layers.zarr" # [str] input volume, CURRENTLY MUST BE ZARR
  input_format: "zarr" # not currently implemented, all data is saved to zarr in the output dir as predictions.zarr
  output_dir: "/home/sean/Desktop/s1_segments/0901/ink_preds"
  output_format: "zarr" # currently does nothing , can only be zarr
  output_type: "np.uint8" # currently does nothing
  output_targets: ["ink"] # required
  load_all: true # optional, false - if true, will load entire input_path volume into memory
  overlap: 0.1   # optional, .01 - the percentage of overlap each patch should have -- this increases your total patch count, so be careful to not go too high

  targets:   # the targets for inference must match a target that will exist in the dictionary your model outputs , and the channels must be equal
    ink:     # to the number of channels your model outputs
      channels: 1
      activation: "none"
      weight: 1